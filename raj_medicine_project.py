# -*- coding: utf-8 -*-
"""Raj Medicine Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1b2XkRn-z_KUtLXqAxAtH_y60h4L9DC-q
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score
import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('/content/medical data.csv')

#   Verify Column Names
print(data.columns)

data.info()

data.isnull().sum()

# Check for missing values in each column
missing_values = data.isnull().sum()

# Display columns with missing values
print(missing_values[missing_values > 0])

# Handling missing values---i use  Data Imputation
# Numeric Columns (DateOfBirth): I impute missing values using the mean or median.
data['DateOfBirth'] = pd.to_datetime(data['DateOfBirth'], errors='coerce')

# Calculate median excluding NaT values (Not a time)
median_date = data['DateOfBirth'].dropna().median()

# Fill missing values with the median date
data['DateOfBirth'].fillna(median_date, inplace=True)

# Categorical Columns (Gender, Symptoms, Causes, Disease, Medicine):I impute missing values with the mode (most frequent value).
categorical_columns = ['Gender', 'Symptoms', 'Causes', 'Disease', 'Medicine']
for column in categorical_columns:
    data[column].fillna(data[column].mode()[0], inplace=True)

# Check for missing values in each column
missing_values = data.isnull().sum()

# Display columns with missing values
print(missing_values[missing_values > 0])

# Handle the missing values in column Name by imputation method
data['Name'].fillna('Unknown', inplace=True)

data.isnull().sum()

# Check for missing values in each column
missing_values = data.isnull().sum()

# Display columns with missing values
print(missing_values[missing_values > 0])

import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Encode categorical variables
label_encoders = {}
for column in data.columns:
    if data[column].dtype == 'object':
        le = LabelEncoder()
        data[column] = le.fit_transform(data[column])
        label_encoders[column] = le

sns.countplot(data=data, x='Medicine')
plt.title('Distribution of Medicine')
plt.show()

# Correlation heatmap for numerical columns
numeric_columns = ['DateOfBirth']
sns.heatmap(data[numeric_columns].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Heatmap')
plt.show()

from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
for column in categorical_columns:
    data[column] = encoder.fit_transform(data[column])

from sklearn.model_selection import train_test_split

X = data.drop('Medicine', axis=1)
y = data['Medicine']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Drop the DateOfBirth column from the datasets
X_train = X_train.drop('DateOfBirth', axis=1)
X_test = X_test.drop('DateOfBirth', axis=1)

# Fit the Decision Tree Classifier
dt_classifier = DecisionTreeClassifier(random_state=42)
dt_classifier.fit(X_train, y_train)

# Make predictions and evaluate
y_pred_dt = dt_classifier.predict(X_test)
accuracy_dt = accuracy_score(y_test, y_pred_dt)
print(f"Decision Tree Accuracy: {accuracy_dt}")

# Random classifier
from sklearn.ensemble import RandomForestClassifier

rf_classifier = RandomForestClassifier(random_state=42)
rf_classifier.fit(X_train, y_train)

y_pred_rf = rf_classifier.predict(X_test)
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest Accuracy: {accuracy_rf}")

# Logistic Regression (for binary classification)
from sklearn.linear_model import LogisticRegression

logreg_classifier = LogisticRegression(random_state=42)
logreg_classifier.fit(X_train, y_train)

y_pred_logreg = logreg_classifier.predict(X_test)
accuracy_logreg = accuracy_score(y_test, y_pred_logreg)
print(f"Logistic Regression Accuracy: {accuracy_logreg}")

# model evaluation---lets Evaluate the models using accuracy, precision, recall, and F1-score.
from sklearn.metrics import classification_report

print("Decision Tree Classification Report:")
print(classification_report(y_test, y_pred_dt))

print("\nRandom Forest Classification Report:")
print(classification_report(y_test, y_pred_rf))

print("\nLogistic Regression Classification Report:")
print(classification_report(y_test, y_pred_logreg))

# Model Interpretation

import matplotlib.pyplot as plt

# lets get feature importances
importances = rf_classifier.feature_importances_

# lets sort feature importances in descending order
indices = np.argsort(importances)[::-1]

# Rearrange feature names so they match the sorted feature importances
names = [X.columns[i] for i in indices]

